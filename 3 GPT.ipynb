{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_utils\n",
    "from transformer_utils import PositionEmbedding, TransformerEncoder, get_masks\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shakespeare data\n",
    "# https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "text = open('input.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2id = {c:i for i, c in enumerate(vocab)}\n",
    "id2char = {i:c for i, c in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2ids(c: str):\n",
    "    return [char2id[i] for i in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inpired by Karpathy's minGPT: https://github.com/karpathy/minGPT\n",
    "\n",
    "# Goal: Pedict next characher\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        \n",
    "        dix = text2ids(chunk)\n",
    "\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        \n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115266\n"
     ]
    }
   ],
   "source": [
    "all_dataset = CharDataset(text, block_size=BLOCK_SIZE)\n",
    "\n",
    "print(len(all_dataset))\n",
    "\n",
    "train, val = random_split(all_dataset, [len(all_dataset)-5000, 5000])\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=256, shuffle=True, num_workers=15)\n",
    "\n",
    "val_loader = DataLoader(val, batch_size=512, shuffle=False, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetitGPT(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, block_size, d_model=512, n_head=8, num_layers=4, d_ffn=1024):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.block_size = block_size\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.register_buffer('causal_mask', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        self.pos_embedding = PositionEmbedding(block_size, d_model, trainable=True)\n",
    "        \n",
    "        self.layers = TransformerEncoder(d_model, n_head, num_layers, d_ffn)\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size, seq_len = x.size()\n",
    "        \n",
    "        assert seq_len <= self.block_size\n",
    "        \n",
    "        # get mask\n",
    "        mask = self.causal_mask[:seq_len, :seq_len]\n",
    "        \n",
    "        # compute word embedding\n",
    "        x = self.embedding(x) # * np.sqrt(self.d_model) (scaling embedding, see Vaswani et al., 2017)\n",
    "        \n",
    "        # add position embedding\n",
    "        x = self.pos_embedding(x)\n",
    "        \n",
    "        # compute tranformer representation\n",
    "        x = self.layers(x, mask)\n",
    "        \n",
    "        # prediction\n",
    "        return self.fc(x)\n",
    "    \n",
    "    def compute_loss(self, x, y):\n",
    "        \n",
    "        pred = self.forward(x)\n",
    "                \n",
    "        loss = F.cross_entropy(pred.view(-1, self.vocab_size), y.view(-1))\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(net: nn.Module, opt: torch.optim, dataloader: torch.utils.data.DataLoader):\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    for param in net.parameters():\n",
    "        device = param.device\n",
    "        break\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    pbar = tqdm(dataloader)\n",
    "    \n",
    "    for x, y in pbar:\n",
    "\n",
    "        net.zero_grad()\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        loss = net.compute_loss(x, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "        \n",
    "        loss_item = loss.item()\n",
    "        \n",
    "        losses.append(loss_item)\n",
    "        \n",
    "        pbar.set_description(f'train_loss = {np.array(losses).mean()}')\n",
    "        \n",
    "    return np.array(losses).mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(net: nn.Module, dataloader: torch.utils.data.DataLoader):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    for param in net.parameters():\n",
    "        device = param.device\n",
    "        break\n",
    "     \n",
    "    losses = []\n",
    "    \n",
    "    for x, y in dataloader:\n",
    "\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        logits = net(x)\n",
    "        \n",
    "        loss = net.compute_loss(x, y)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return np.array(losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {'vocab_size': len(vocab), 'block_size': BLOCK_SIZE, 'num_layers':1, 'd_model':256, 'n_head':4}\n",
    "\n",
    "model = PetitGPT(**model_args).cuda()\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_loss = 1.8799111138353266: 100%|██████████| 4337/4337 [06:07<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5542835474014283\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "\n",
    "n_epoch = 1\n",
    "\n",
    "for _ in range(n_epoch):\n",
    "    \n",
    "    train_one_epoch(model, opt, train_loader)\n",
    "    \n",
    "    print(validate(model, val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "model.cpu()\n",
    "model.eval()\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('saved_models'):\n",
    "    os.mkdir('saved_models')\n",
    "    \n",
    "path = 'saved_models/petit_gpt.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_save = {}\n",
    "\n",
    "dic_save['model_args'] = model_args\n",
    "\n",
    "dic_save['model_weights'] = model.state_dict()\n",
    "\n",
    "dic_save['opt_state'] = opt.state_dict()\n",
    "\n",
    "torch.save(dic_save, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load save\n",
    "dic_save = torch.load(path)\n",
    "\n",
    "# initialise a model with the model argument\n",
    "model = PetitGPT(**dic_save['model_args'])\n",
    "\n",
    "# load model weigths\n",
    "model.load_state_dict(dic_save['model_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_sampling(logits, k):\n",
    "    \n",
    "    logits = logits.squeeze()\n",
    "    \n",
    "    topk = torch.topk(logits, k)\n",
    "    \n",
    "    probs, indices = torch.softmax(topk.values, dim=0).numpy(), topk.indices.numpy()   \n",
    "    \n",
    "    return np.random.choice(indices, p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(start: str='The sky', max_len: int=1000, topk: int=5):\n",
    "    \n",
    "    k = text2ids(start)\n",
    "    \n",
    "    while len(k) < max_len:\n",
    "        \n",
    "        x_k = k[-model.block_size:]\n",
    "        \n",
    "        x = torch.LongTensor(x_k).unsqueeze(0)\n",
    "        \n",
    "        out = model.forward(x)[0, -1, :]\n",
    "        \n",
    "        out = topk_sampling(out, k=topk)\n",
    "        \n",
    "        k.append(out.item())\n",
    "        \n",
    "    return ''.join([id2char[s] for s in k])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RNN decoding\n",
    "# x_t, h_t => x_t+1, h_t+1\n",
    "\n",
    "## Transformers\n",
    "# x_t => x_t+1\n",
    "# [x_t, x_t+1] => [x_t+1, x_t+2]\n",
    "# [x_t, x_t+1, x_t+2] => [x_t+1, x_t+2, x_t+3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky,\n",
      "In there stonger with they, thout a sunstred.\n",
      "\n",
      "POLIXENES:\n",
      "Ay me son, my hone to me hopessage.\n",
      "Within my for the me, and with thought thou art of marrance;\n",
      "What with a betiest this follow.\n",
      "\n",
      "LEONTENIUS:\n",
      "Why mean with the set my horse, and we the hand.\n",
      "\n",
      "KING RICHARD II:\n",
      "Thoughter than whose hear the speak to holdier their son\n",
      "Then this him a many a marcians thou with her with a son my this all at and;\n",
      "And it with the house, thou art into me,\n",
      "Become and my fless when with of all and my he straith\n",
      "And more of thee at it in me, with have me;\n",
      "And to made is, which there town, who confest the hate\n",
      "Some formit hat to they to blood, and have subjesting me their hels. The have thoughts will think,\n",
      "Thou and so his far\n",
      "Who have show for to thich\n",
      "With honours to house, what is those on a trough this follower him;\n",
      "A may, thou she four toody to make hearth that has\n",
      "and traiged; whed fort the what stile him that\n",
      "To the procling, to say tide taught welcome for hate one shoriold tender,\n",
      "And that\n"
     ]
    }
   ],
   "source": [
    "print(generate('The sky', max_len=1000, topk=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_UZA",
   "language": "python",
   "name": "py37_uza"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
